{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "priority-sort-task-notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC8G_atE7XVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install dnc visdom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psw7TZ8O7Ghp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import getopt\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import argparse\n",
        "from visdom import Visdom\n",
        "\n",
        "import json\n",
        "\n",
        "sys.path.insert(0, os.path.join('..', '..'))\n",
        "\n",
        "import torch as T\n",
        "from torch.autograd import Variable as var\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from dnc.dnc import DNC\n",
        "from dnc.sdnc import SDNC\n",
        "from dnc.sam import SAM\n",
        "from dnc.util import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8okGcbL37Gh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def llprint(message):\n",
        "    sys.stdout.write(message)\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "def UNgenerate_data(batch_size, length, size, cuda=-1):\n",
        "\n",
        "    print(batch_size, length, size)\n",
        "  \n",
        "    input_data = np.zeros((batch_size, 2 * length + 1, size), dtype=np.float32)\n",
        "    target_output = np.zeros((batch_size, 2 * length + 1, size), dtype=np.float32)\n",
        "    \n",
        "    sequence = np.random.binomial(1, 0.5, (batch_size, length, size - 1))\n",
        "\n",
        "    input_data[:, :length, :size - 1] = sequence\n",
        "    input_data[:, length, -1] = 1  # the end symbol\n",
        "    target_output[:, length + 1:, :size - 1] = sequence\n",
        "\n",
        "    input_data = T.from_numpy(input_data)\n",
        "    target_output = T.from_numpy(target_output)\n",
        "    if cuda != -1:\n",
        "        input_data = input_data.cuda()\n",
        "        target_output = target_output.cuda()\n",
        "    return var(input_data), var(target_output)\n",
        "\n",
        "\n",
        "def generate_data(batch_size, length, size, cuda=-1):\n",
        "  \n",
        "        seq = np.random.binomial(1, 0.5, (batch_size, length, size-1))\n",
        "        seq = torch.from_numpy(seq)\n",
        "\n",
        "        # The input includes an additional channel used for the delimiter\n",
        "        inp = torch.zeros(batch_size, length, size)\n",
        "  \n",
        "        # Add priority number (just a single one drawn from the uniform distribution)\n",
        "        priority = np.random.uniform(-1, 1, (batch_size, length, 1))\n",
        "        priority = torch.from_numpy(priority)\n",
        "        \n",
        "        # Construct the input vectors\n",
        "        inp[:, :length, :(size-1)] = seq\n",
        "        inp[:, :length, (size-1):] = priority  # priority\n",
        "        \n",
        "        # Add the delimiter\n",
        "        #inp_delim = torch.zeros(batch_size, length+1, size+2)        \n",
        "        #inp_delim[:, :(length), :(size+1)] = inp\n",
        "        #inp_delim[:, length, size+1] = 1.0  # delimiter in our control channel\n",
        "        \n",
        "        # Construct the output which will be a sorterd version of\n",
        "        # the sequences given by looking at the priority\n",
        "        #outp = inp_delim.numpy()\n",
        "        outp = inp.numpy()\n",
        "        \n",
        "        # Strip all the binary vectors into a list\n",
        "        # and sort the list by looking at the last column\n",
        "        # (which will contain the priority)\n",
        "        temp = []\n",
        "        for i in range(len(outp)):\n",
        "            temp.append(outp[i][0])\n",
        "        #temp = [e[:size+1] for e in temp]\n",
        "        temp.sort(key=lambda x: x[size-1], reverse=True) # Sort elements descending order\n",
        "                \n",
        "        # Keep only the highest entries as specified in the paper.\n",
        "        # This means that for 20 entries we want to predict only the highest 16.\n",
        "        # This will be done only if a sequence is larger than 4 elements.\n",
        "        #if len(temp) > 4:\n",
        "        #    del temp[-4:]\n",
        "\n",
        "        # FIXME\n",
        "        # Ugly hack to present the tensor structure as the one\n",
        "        # required by the framework\n",
        "        layer = []\n",
        "        for i in range(len(temp)):\n",
        "            tmp_layer = []\n",
        "            tmp_layer.append(np.array(temp[i]))\n",
        "            layer.append(tmp_layer)\n",
        "\n",
        "        # Convert everything to numpy and to a tensor\n",
        "        outp = torch.from_numpy(np.array(layer))\n",
        "        \n",
        "        if cuda != -1:\n",
        "          #inp_delim = inp_delim.cuda()\n",
        "          inp = inp.cuda()\n",
        "          outp = outp.cuda()\n",
        "\n",
        "        return var(inp.float()), var(outp.float())\n",
        "  \n",
        "\n",
        "def criterion(predictions, targets):\n",
        "    return T.mean(\n",
        "      -1 * F.logsigmoid(predictions) * (targets) - T.log(1 - F.sigmoid(predictions) + 1e-9) * (1 - targets)\n",
        "  )\n",
        "  \n",
        "  \n",
        "def binarize_array(array):\n",
        "  binar = lambda x: 0 if x < 0.5 else 1\n",
        "  bfunc = np.vectorize(binar)\n",
        "  return np.array(bfunc(array))\n",
        "\n",
        "def binarize_matrix(matrix):\n",
        "  result = []\n",
        "  binar = lambda x: 0 if x < 0.5 else 1\n",
        "  bfunc = np.vectorize(binar)\n",
        "  for r in matrix:\n",
        "    result.append(np.array(bfunc(r)))\n",
        "  return np.array(result)\n",
        "  \n",
        "def compute_cost(model, batch_size, length, size, mhx, cuda=-1):\n",
        "    \n",
        "  input_data, target_out = generate_data(batch_size, length, size, cuda=-1)\n",
        "  \n",
        "  if cuda != -1:\n",
        "    input_data = input_data.cuda()\n",
        "    target_out = target_out.cuda()\n",
        "  \n",
        "  output, (chx, mhx, rv) = model(input_data)\n",
        "  \n",
        "  # Binarize the result\n",
        "  y_out_binarized = []\n",
        "  for t in output:\n",
        "    y_out_binarized.append((t>0.5).data.cpu().numpy())\n",
        "  y_out_binarized = T.from_numpy(np.array(y_out_binarized)) \n",
        "  \n",
        "  # Binarize the original output\n",
        "  target_output = []\n",
        "  for t in target_out:\n",
        "    target_output.append((t>0.5).data.cpu().numpy())\n",
        "  target_output = T.from_numpy(np.array(target_output)) \n",
        "  \n",
        "  # The cost is the number of error bits per sequence\n",
        "  cost = torch.sum(torch.abs(y_out_binarized.cpu().float() - target_output.cpu().float()))/batch_size  \n",
        "  \n",
        "  return cost                 \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMHzs9HK7Gh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "args = Namespace(input_size=9, rnn_type=\"lstm\", nhid=100, dropout=0, memory_type=\"dnc\", nlayer=1, nhlayer=2,\n",
        "                 lr=1e-4, optim=\"adam\", clip=10, batch_size=1, mem_size=20, mem_slot=128, read_heads=5,\n",
        "                 sparse_reads=10, temporal_reads=2, sequence_max_length=2, curriculum_increment=0, curriculum_freq=1000,\n",
        "                 cuda=0, iterations=200, summarize_freq=100, check_freq=100, visdom=False)\n",
        "if args.visdom:\n",
        "    viz = Visdom()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th4hxzwQ7GiG",
        "colab_type": "code",
        "outputId": "7d3ab8a4-d148-440c-f74c-0b28c213f749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        }
      },
      "source": [
        "dirname = os.path.dirname(\".\")\n",
        "ckpts_dir = os.path.join(dirname, 'checkpoints')\n",
        "if not os.path.isdir(ckpts_dir):\n",
        "    os.mkdir(ckpts_dir)\n",
        "\n",
        "batch_size = args.batch_size\n",
        "sequence_max_length = args.sequence_max_length\n",
        "iterations = args.iterations\n",
        "summarize_freq = args.summarize_freq\n",
        "check_freq = args.check_freq\n",
        "\n",
        "# input_size = output_size = args.input_size\n",
        "mem_slot = args.mem_slot\n",
        "mem_size = args.mem_size\n",
        "read_heads = args.read_heads\n",
        "\n",
        "if args.memory_type == 'dnc':\n",
        "    rnn = DNC(\n",
        "    input_size=args.input_size,\n",
        "    hidden_size=args.nhid,\n",
        "    rnn_type=args.rnn_type,\n",
        "    num_layers=args.nlayer,\n",
        "    num_hidden_layers=args.nhlayer,\n",
        "    dropout=args.dropout,\n",
        "    nr_cells=mem_slot,\n",
        "    cell_size=mem_size,\n",
        "    read_heads=read_heads,\n",
        "    gpu_id=args.cuda,\n",
        "    debug=args.visdom,\n",
        "    batch_first=True,\n",
        "    independent_linears=True\n",
        ")\n",
        "elif args.memory_type == 'sdnc':\n",
        "    rnn = SDNC(\n",
        "    input_size=args.input_size,\n",
        "    hidden_size=args.nhid,\n",
        "    rnn_type=args.rnn_type,\n",
        "    num_layers=args.nlayer,\n",
        "    num_hidden_layers=args.nhlayer,\n",
        "    dropout=args.dropout,\n",
        "    nr_cells=mem_slot,\n",
        "    cell_size=mem_size,\n",
        "    sparse_reads=args.sparse_reads,\n",
        "    temporal_reads=args.temporal_reads,\n",
        "    read_heads=args.read_heads,\n",
        "    gpu_id=args.cuda,\n",
        "    debug=args.visdom,\n",
        "    batch_first=True,\n",
        "    independent_linears=False\n",
        ")\n",
        "elif args.memory_type == 'sam':\n",
        "    rnn = SAM(\n",
        "    input_size=args.input_size,\n",
        "    hidden_size=args.nhid,\n",
        "    rnn_type=args.rnn_type,\n",
        "    num_layers=args.nlayer,\n",
        "    num_hidden_layers=args.nhlayer,\n",
        "    dropout=args.dropout,\n",
        "    nr_cells=mem_slot,\n",
        "    cell_size=mem_size,\n",
        "    sparse_reads=args.sparse_reads,\n",
        "    read_heads=args.read_heads,\n",
        "    gpu_id=args.cuda,\n",
        "    debug=args.visdom,\n",
        "    batch_first=True,\n",
        "    independent_linears=False\n",
        ")\n",
        "else:\n",
        "    raise Exception('Not recognized type of memory')\n",
        "\n",
        "print(rnn)\n",
        "# register_nan_checks(rnn)\n",
        "\n",
        "if args.cuda != -1:\n",
        "    rnn = rnn.cuda(args.cuda)\n",
        "\n",
        "if args.optim == 'adam':\n",
        "    optimizer = optim.Adam(rnn.parameters(), lr=args.lr, eps=1e-9, betas=[0.9, 0.98]) # 0.0001\n",
        "elif args.optim == 'adamax':\n",
        "    optimizer = optim.Adamax(rnn.parameters(), lr=args.lr, eps=1e-9, betas=[0.9, 0.98]) # 0.0001\n",
        "elif args.optim == 'rmsprop':\n",
        "    optimizer = optim.RMSprop(rnn.parameters(), lr=args.lr, momentum=0.9, eps=1e-10) # 0.0001\n",
        "elif args.optim == 'sgd':\n",
        "    optimizer = optim.SGD(rnn.parameters(), lr=args.lr) # 0.01\n",
        "elif args.optim == 'adagrad':\n",
        "    optimizer = optim.Adagrad(rnn.parameters(), lr=args.lr)\n",
        "elif args.optim == 'adadelta':\n",
        "    optimizer = optim.Adadelta(rnn.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "# List for keeping useful data    \n",
        "costs = []\n",
        "last_save_losses=[]\n",
        "seq_lengths = []\n",
        "\n",
        "(chx, mhx, rv) = (None, None, None)\n",
        "for epoch in range(iterations + 1):\n",
        "    llprint(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #random_length = np.random.randint(1, sequence_max_length + 1)\n",
        "    random_length=sequence_max_length\n",
        "    \n",
        "    input_data, target_output = generate_data(batch_size, random_length, args.input_size, args.cuda)\n",
        "\n",
        "    if rnn.debug:\n",
        "        output, (chx, mhx, rv), v = rnn(input_data, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
        "    else:\n",
        "        output, (chx, mhx, rv) = rnn(input_data, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
        "\n",
        "    loss = criterion((output), target_output)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    T.nn.utils.clip_grad_norm_(rnn.parameters(), args.clip)\n",
        "    optimizer.step()\n",
        "    loss_value = loss.item()\n",
        "\n",
        "    summarize = (epoch % summarize_freq == 0)\n",
        "    take_checkpoint = (epoch != 0) and (epoch % check_freq == 0)\n",
        "    increment_curriculum = (epoch != 0) and (epoch % args.curriculum_freq == 0)\n",
        "\n",
        "    # detach memory from graph\n",
        "    mhx = { k : (v.detach() if isinstance(v, var) else v) for k, v in mhx.items() }\n",
        "\n",
        "    # Save loss value\n",
        "    last_save_losses.append(loss_value)\n",
        "\n",
        "    # Save cost value\n",
        "    costs.append(compute_cost(rnn, batch_size, random_length, args.input_size, mhx, args.cuda).item())\n",
        "    \n",
        "    # Save sequence length\n",
        "    seq_lengths.append(args.input_size)\n",
        "    \n",
        "    if summarize:\n",
        "        loss = np.mean(last_save_losses)\n",
        "        cost = np.mean(costs)\n",
        "      # print(input_data)\n",
        "      # print(\"1111111111111111111111111111111111111111111111\")\n",
        "      # print(target_output)\n",
        "      # print('2222222222222222222222222222222222222222222222')\n",
        "      # print(F.relu6(output))\n",
        "        llprint(\"\\n\\tAvg. Logistic Loss: %.4f\" % (loss))\n",
        "        llprint(\"\\n\\tAvg. Cost: %4f\\n\"% (cost))\n",
        "        if np.isnan(loss):\n",
        "            raise Exception('nan Loss')\n",
        "\n",
        "    if summarize and rnn.debug:\n",
        "        loss = np.mean(last_save_losses)\n",
        "      # print(input_data)\n",
        "      # print(\"1111111111111111111111111111111111111111111111\")\n",
        "      # print(target_output)\n",
        "      # print('2222222222222222222222222222222222222222222222')\n",
        "      # print(F.relu6(output))\n",
        "        #last_save_losses = []\n",
        "    \n",
        "    if args.visdom:\n",
        "        if args.memory_type == 'dnc':\n",
        "            viz.heatmap(\n",
        "                v['memory'],\n",
        "                opts=dict(\n",
        "                    xtickstep=10,\n",
        "                    ytickstep=2,\n",
        "                    title='Memory, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                    ylabel='layer * time',\n",
        "                    xlabel='mem_slot * mem_size'\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if args.memory_type == 'dnc':\n",
        "            viz.heatmap(\n",
        "                v['link_matrix'][-1].reshape(args.mem_slot, args.mem_slot),\n",
        "                opts=dict(\n",
        "                    xtickstep=10,\n",
        "                    ytickstep=2,\n",
        "                    title='Link Matrix, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                    ylabel='mem_slot',\n",
        "                    xlabel='mem_slot'\n",
        "                )\n",
        "            )\n",
        "        elif args.memory_type == 'sdnc':\n",
        "            viz.heatmap(\n",
        "                v['link_matrix'][-1].reshape(args.mem_slot, -1),\n",
        "                opts=dict(\n",
        "                    xtickstep=10,\n",
        "                    ytickstep=2,\n",
        "                    title='Link Matrix, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                    ylabel='mem_slot',\n",
        "                    xlabel='mem_slot'\n",
        "                )\n",
        "            )\n",
        "\n",
        "            viz.heatmap(\n",
        "                v['rev_link_matrix'][-1].reshape(args.mem_slot, -1),\n",
        "                opts=dict(\n",
        "                    xtickstep=10,\n",
        "                    ytickstep=2,\n",
        "                    title='Reverse Link Matrix, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                    ylabel='mem_slot',\n",
        "                    xlabel='mem_slot'\n",
        "                )\n",
        "            )\n",
        "\n",
        "        elif args.memory_type == 'sdnc' or args.memory_type == 'dnc':\n",
        "            viz.heatmap(\n",
        "                v['precedence'],\n",
        "                opts=dict(\n",
        "                    xtickstep=10,\n",
        "                    ytickstep=2,\n",
        "                    title='Precedence, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                    ylabel='layer * time',\n",
        "                    xlabel='mem_slot'\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if args.memory_type == 'sdnc':\n",
        "            viz.heatmap(\n",
        "                v['read_positions'],\n",
        "                opts=dict(\n",
        "                    xtickstep=10,\n",
        "                    ytickstep=2,\n",
        "                    title='Read Positions, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                    ylabel='layer * time',\n",
        "                    xlabel='mem_slot'\n",
        "                )\n",
        "            )\n",
        "\n",
        "            viz.heatmap(\n",
        "              v['read_weights'],\n",
        "              opts=dict(\n",
        "                  xtickstep=10,\n",
        "                  ytickstep=2,\n",
        "                  title='Read Weights, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                  ylabel='layer * time',\n",
        "                  xlabel='nr_read_heads * mem_slot'\n",
        "              )\n",
        "          )\n",
        "\n",
        "            viz.heatmap(\n",
        "              v['write_weights'],\n",
        "              opts=dict(\n",
        "                  xtickstep=10,\n",
        "                  ytickstep=2,\n",
        "                  title='Write Weights, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                  ylabel='layer * time',\n",
        "                  xlabel='mem_slot'\n",
        "              )\n",
        "          )\n",
        "\n",
        "            viz.heatmap(\n",
        "              v['usage_vector'] if args.memory_type == 'dnc' else v['usage'],\n",
        "              opts=dict(\n",
        "                  xtickstep=10,\n",
        "                  ytickstep=2,\n",
        "                  title='Usage Vector, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
        "                  ylabel='layer * time',\n",
        "                  xlabel='mem_slot'\n",
        "              )\n",
        "          )\n",
        "\n",
        "    if increment_curriculum:\n",
        "        sequence_max_length = sequence_max_length + args.curriculum_increment\n",
        "        print(\"Increasing max length to \" + str(sequence_max_length))\n",
        "\n",
        "    if take_checkpoint:\n",
        "        check_ptr = os.path.join(ckpts_dir, 'step_{}.pth'.format(epoch))\n",
        "        llprint(\"\\nSaving Checkpoint to {}\\n\".format(check_ptr))\n",
        "        cur_weights = rnn.state_dict()\n",
        "        T.save(cur_weights, check_ptr)\n",
        "        \n",
        "        # Save data\n",
        "        performance_data_path = os.path.join(ckpts_dir, 'results_{}.csv'.format(epoch))\n",
        "        content = {\n",
        "          \"loss\": last_save_losses,\n",
        "          \"cost\": costs,\n",
        "          \"seq_lengths\": seq_lengths\n",
        "        }\n",
        "        f = open(performance_data_path, 'w+')\n",
        "        f.write(json.dumps(content))\n",
        "        f.close()\n",
        "        \n",
        "        llprint(\"Done!\\n\")"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------\n",
            "DNC(9, 100, nr_cells=128, read_heads=5, cell_size=20, gpu_id=0, independent_linears=True)\n",
            "DNC(\n",
            "  (lstm_layer_0): LSTM(109, 100, num_layers=2, batch_first=True)\n",
            "  (rnn_layer_memory_shared): Memory(\n",
            "    (read_keys_transform): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (read_strengths_transform): Linear(in_features=100, out_features=5, bias=True)\n",
            "    (write_key_transform): Linear(in_features=100, out_features=20, bias=True)\n",
            "    (write_strength_transform): Linear(in_features=100, out_features=1, bias=True)\n",
            "    (erase_vector_transform): Linear(in_features=100, out_features=20, bias=True)\n",
            "    (write_vector_transform): Linear(in_features=100, out_features=20, bias=True)\n",
            "    (free_gates_transform): Linear(in_features=100, out_features=5, bias=True)\n",
            "    (allocation_gate_transform): Linear(in_features=100, out_features=1, bias=True)\n",
            "    (write_gate_transform): Linear(in_features=100, out_features=1, bias=True)\n",
            "    (read_modes_transform): Linear(in_features=100, out_features=15, bias=True)\n",
            "  )\n",
            "  (output): Linear(in_features=200, out_features=9, bias=True)\n",
            ")\n",
            "----------------------------------------\n",
            "\n",
            "\rIteration 0/200\n",
            "\tAvg. Logistic Loss: 0.6869\n",
            "\tAvg. Cost: 4.000000\n",
            "Iteration 100/200\n",
            "\tAvg. Logistic Loss: 0.6887\n",
            "\tAvg. Cost: 8.336634\n",
            "\n",
            "Saving Checkpoint to checkpoints/step_100.pth\n",
            "Done!\n",
            "Iteration 200/200\n",
            "\tAvg. Logistic Loss: 0.6869\n",
            "\tAvg. Cost: 8.308458\n",
            "\n",
            "Saving Checkpoint to checkpoints/step_200.pth\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aUbRYJ-7GiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLXMGugbJ6uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(int((iterations + 1) / 10)):\n",
        "    llprint(\"\\nIteration %d/%d\" % (i, iterations))\n",
        "    # We test now the learned generalization using sequence_max_length examples\n",
        "    random_length = np.random.randint(2, sequence_max_length * 10 + 1)\n",
        "    input_data, target_output, loss_weights = generate_data(random_length, input_size)\n",
        "\n",
        "    if rnn.debug:\n",
        "        output, (chx, mhx, rv), v = rnn(input_data, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
        "    else:\n",
        "        output, (chx, mhx, rv) = rnn(input_data, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
        "\n",
        "    output = output[:, -1, :].sum().data.cpu().numpy()[0]\n",
        "    target_output = target_output.sum().data.cpu().numpy()\n",
        "\n",
        "try:\n",
        "    print(\"\\nReal value: \", ' = ' + str(int(target_output[0])))\n",
        "    print(\"Predicted:  \", ' = ' + str(int(output // 1)) + \" [\" + str(output) + \"]\")\n",
        "except Exception as e:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}