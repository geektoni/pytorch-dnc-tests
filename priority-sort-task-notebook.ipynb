{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import getopt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "from visdom import Visdom\n",
    "\n",
    "sys.path.insert(0, os.path.join('..', '..'))\n",
    "\n",
    "import torch as T\n",
    "from torch.autograd import Variable as var\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from dnc.dnc import DNC\n",
    "from dnc.sdnc import SDNC\n",
    "from dnc.sam import SAM\n",
    "from dnc.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llprint(message):\n",
    "    sys.stdout.write(message)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def generate_data(batch_size, length, size, cuda=-1):\n",
    "\n",
    "    input_data = np.zeros((batch_size, 2 * length + 1, size), dtype=np.float32)\n",
    "    target_output = np.zeros((batch_size, 2 * length + 1, size), dtype=np.float32)\n",
    "    \n",
    "    sequence = np.random.binomial(1, 0.5, (batch_size, length, size - 1))\n",
    "\n",
    "    input_data[:, :length, :size - 1] = sequence\n",
    "    input_data[:, length, -1] = 1  # the end symbol\n",
    "    target_output[:, length + 1:, :size - 1] = sequence\n",
    "\n",
    "    input_data = T.from_numpy(input_data)\n",
    "    target_output = T.from_numpy(target_output)\n",
    "    if cuda != -1:\n",
    "        input_data = input_data.cuda()\n",
    "        target_output = target_output.cuda()\n",
    "    return var(input_data), var(target_output)\n",
    "\n",
    "\n",
    "def criterion(predictions, targets):\n",
    "    return T.mean(\n",
    "      -1 * F.logsigmoid(predictions) * (targets) - T.log(1 - F.sigmoid(predictions) + 1e-9) * (1 - targets)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(input_size=6, rnn_type=\"lstm\", nhid=64, dropout=0, memory_type=\"dnc\", nlayer=1, nhlayer=2,\n",
    "                 lr=1e-4, optim=\"adam\", clip=10, batch_size=100, mem_size=20, mem_slot=16, read_heads=4,\n",
    "                 sparse_reads=10, temporal_reads=2, sequence_max_length=4, curriculum_increment=0, curriculum_freq=1000,\n",
    "                 cuda=-1, iterations=100000, summarize_freq=100, check_freq=100, visdom=False)\n",
    "if args.visdom:\n",
    "    viz = Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "DNC(6, 64, nr_cells=16, read_heads=4, cell_size=20, independent_linears=True)\n",
      "DNC(\n",
      "  (lstm_layer_0): LSTM(86, 64, num_layers=2, batch_first=True)\n",
      "  (rnn_layer_memory_shared): Memory(\n",
      "    (read_keys_transform): Linear(in_features=64, out_features=80, bias=True)\n",
      "    (read_strengths_transform): Linear(in_features=64, out_features=4, bias=True)\n",
      "    (write_key_transform): Linear(in_features=64, out_features=20, bias=True)\n",
      "    (write_strength_transform): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (erase_vector_transform): Linear(in_features=64, out_features=20, bias=True)\n",
      "    (write_vector_transform): Linear(in_features=64, out_features=20, bias=True)\n",
      "    (free_gates_transform): Linear(in_features=64, out_features=4, bias=True)\n",
      "    (allocation_gate_transform): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (write_gate_transform): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (read_modes_transform): Linear(in_features=64, out_features=12, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=144, out_features=6, bias=True)\n",
      ")\n",
      "----------------------------------------\n",
      "\n",
      "Iteration 0/100000\n",
      "\tAvg. Logistic Loss: 0.6933\n",
      "Iteration 100/100000\n",
      "\tAvg. Logistic Loss: 0.6783\n",
      "\n",
      "Saving Checkpoint ... Done!\n",
      "Iteration 200/100000\n",
      "\tAvg. Logistic Loss: 0.6536\n",
      "\n",
      "Saving Checkpoint ... Done!\n",
      "Iteration 300/100000\n",
      "\tAvg. Logistic Loss: 0.6274\n",
      "\n",
      "Saving Checkpoint ... Done!\n",
      "Iteration 400/100000\n",
      "\tAvg. Logistic Loss: 0.5998\n",
      "\n",
      "Saving Checkpoint ... Done!\n",
      "Iteration 465/100000"
     ]
    }
   ],
   "source": [
    "dirname = os.path.dirname(\".\")\n",
    "ckpts_dir = os.path.join(dirname, 'checkpoints')\n",
    "if not os.path.isdir(ckpts_dir):\n",
    "    os.mkdir(ckpts_dir)\n",
    "\n",
    "batch_size = args.batch_size\n",
    "sequence_max_length = args.sequence_max_length\n",
    "iterations = args.iterations\n",
    "summarize_freq = args.summarize_freq\n",
    "check_freq = args.check_freq\n",
    "\n",
    "# input_size = output_size = args.input_size\n",
    "mem_slot = args.mem_slot\n",
    "mem_size = args.mem_size\n",
    "read_heads = args.read_heads\n",
    "\n",
    "if args.memory_type == 'dnc':\n",
    "    rnn = DNC(\n",
    "    input_size=args.input_size,\n",
    "    hidden_size=args.nhid,\n",
    "    rnn_type=args.rnn_type,\n",
    "    num_layers=args.nlayer,\n",
    "    num_hidden_layers=args.nhlayer,\n",
    "    dropout=args.dropout,\n",
    "    nr_cells=mem_slot,\n",
    "    cell_size=mem_size,\n",
    "    read_heads=read_heads,\n",
    "    gpu_id=args.cuda,\n",
    "    debug=args.visdom,\n",
    "    batch_first=True,\n",
    "    independent_linears=True\n",
    ")\n",
    "elif args.memory_type == 'sdnc':\n",
    "    rnn = SDNC(\n",
    "    input_size=args.input_size,\n",
    "    hidden_size=args.nhid,\n",
    "    rnn_type=args.rnn_type,\n",
    "    num_layers=args.nlayer,\n",
    "    num_hidden_layers=args.nhlayer,\n",
    "    dropout=args.dropout,\n",
    "    nr_cells=mem_slot,\n",
    "    cell_size=mem_size,\n",
    "    sparse_reads=args.sparse_reads,\n",
    "    temporal_reads=args.temporal_reads,\n",
    "    read_heads=args.read_heads,\n",
    "    gpu_id=args.cuda,\n",
    "    debug=args.visdom,\n",
    "    batch_first=True,\n",
    "    independent_linears=False\n",
    ")\n",
    "elif args.memory_type == 'sam':\n",
    "    rnn = SAM(\n",
    "    input_size=args.input_size,\n",
    "    hidden_size=args.nhid,\n",
    "    rnn_type=args.rnn_type,\n",
    "    num_layers=args.nlayer,\n",
    "    num_hidden_layers=args.nhlayer,\n",
    "    dropout=args.dropout,\n",
    "    nr_cells=mem_slot,\n",
    "    cell_size=mem_size,\n",
    "    sparse_reads=args.sparse_reads,\n",
    "    read_heads=args.read_heads,\n",
    "    gpu_id=args.cuda,\n",
    "    debug=args.visdom,\n",
    "    batch_first=True,\n",
    "    independent_linears=False\n",
    ")\n",
    "else:\n",
    "    raise Exception('Not recognized type of memory')\n",
    "\n",
    "print(rnn)\n",
    "# register_nan_checks(rnn)\n",
    "\n",
    "if args.cuda != -1:\n",
    "    rnn = rnn.cuda(args.cuda)\n",
    "\n",
    "last_save_losses = []\n",
    "\n",
    "if args.optim == 'adam':\n",
    "    optimizer = optim.Adam(rnn.parameters(), lr=args.lr, eps=1e-9, betas=[0.9, 0.98]) # 0.0001\n",
    "elif args.optim == 'adamax':\n",
    "    optimizer = optim.Adamax(rnn.parameters(), lr=args.lr, eps=1e-9, betas=[0.9, 0.98]) # 0.0001\n",
    "elif args.optim == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(rnn.parameters(), lr=args.lr, momentum=0.9, eps=1e-10) # 0.0001\n",
    "elif args.optim == 'sgd':\n",
    "    optimizer = optim.SGD(rnn.parameters(), lr=args.lr) # 0.01\n",
    "elif args.optim == 'adagrad':\n",
    "    optimizer = optim.Adagrad(rnn.parameters(), lr=args.lr)\n",
    "elif args.optim == 'adadelta':\n",
    "    optimizer = optim.Adadelta(rnn.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "(chx, mhx, rv) = (None, None, None)\n",
    "for epoch in range(iterations + 1):\n",
    "    llprint(\"\\rIteration {ep}/{tot}\".format(ep=epoch, tot=iterations))\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    random_length = np.random.randint(1, sequence_max_length + 1)\n",
    "\n",
    "    input_data, target_output = generate_data(batch_size, random_length, args.input_size, args.cuda)\n",
    "\n",
    "    if rnn.debug:\n",
    "        output, (chx, mhx, rv), v = rnn(input_data, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "    else:\n",
    "        output, (chx, mhx, rv) = rnn(input_data, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "\n",
    "    loss = criterion((output), target_output)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    T.nn.utils.clip_grad_norm_(rnn.parameters(), args.clip)\n",
    "    optimizer.step()\n",
    "    loss_value = loss.item()\n",
    "\n",
    "    summarize = (epoch % summarize_freq == 0)\n",
    "    take_checkpoint = (epoch != 0) and (epoch % check_freq == 0)\n",
    "    increment_curriculum = (epoch != 0) and (epoch % args.curriculum_freq == 0)\n",
    "\n",
    "    # detach memory from graph\n",
    "    mhx = { k : (v.detach() if isinstance(v, var) else v) for k, v in mhx.items() }\n",
    "\n",
    "    last_save_losses.append(loss_value)\n",
    "\n",
    "    if summarize:\n",
    "        loss = np.mean(last_save_losses)\n",
    "      # print(input_data)\n",
    "      # print(\"1111111111111111111111111111111111111111111111\")\n",
    "      # print(target_output)\n",
    "      # print('2222222222222222222222222222222222222222222222')\n",
    "      # print(F.relu6(output))\n",
    "        llprint(\"\\n\\tAvg. Logistic Loss: %.4f\\n\" % (loss))\n",
    "        if np.isnan(loss):\n",
    "            raise Exception('nan Loss')\n",
    "\n",
    "    if summarize and rnn.debug:\n",
    "        loss = np.mean(last_save_losses)\n",
    "      # print(input_data)\n",
    "      # print(\"1111111111111111111111111111111111111111111111\")\n",
    "      # print(target_output)\n",
    "      # print('2222222222222222222222222222222222222222222222')\n",
    "      # print(F.relu6(output))\n",
    "        last_save_losses = []\n",
    "    \n",
    "    if args.visdom:\n",
    "        if args.memory_type == 'dnc':\n",
    "            viz.heatmap(\n",
    "                v['memory'],\n",
    "                opts=dict(\n",
    "                    xtickstep=10,\n",
    "                    ytickstep=2,\n",
    "                    title='Memory, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                    ylabel='layer * time',\n",
    "                    xlabel='mem_slot * mem_size'\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if args.memory_type == 'dnc':\n",
    "            viz.heatmap(\n",
    "                v['link_matrix'][-1].reshape(args.mem_slot, args.mem_slot),\n",
    "                opts=dict(\n",
    "                    xtickstep=10,\n",
    "                    ytickstep=2,\n",
    "                    title='Link Matrix, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                    ylabel='mem_slot',\n",
    "                    xlabel='mem_slot'\n",
    "                )\n",
    "            )\n",
    "        elif args.memory_type == 'sdnc':\n",
    "            viz.heatmap(\n",
    "                v['link_matrix'][-1].reshape(args.mem_slot, -1),\n",
    "                opts=dict(\n",
    "                    xtickstep=10,\n",
    "                    ytickstep=2,\n",
    "                    title='Link Matrix, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                    ylabel='mem_slot',\n",
    "                    xlabel='mem_slot'\n",
    "                )\n",
    "            )\n",
    "\n",
    "            viz.heatmap(\n",
    "                v['rev_link_matrix'][-1].reshape(args.mem_slot, -1),\n",
    "                opts=dict(\n",
    "                    xtickstep=10,\n",
    "                    ytickstep=2,\n",
    "                    title='Reverse Link Matrix, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                    ylabel='mem_slot',\n",
    "                    xlabel='mem_slot'\n",
    "                )\n",
    "            )\n",
    "\n",
    "        elif args.memory_type == 'sdnc' or args.memory_type == 'dnc':\n",
    "            viz.heatmap(\n",
    "                v['precedence'],\n",
    "                opts=dict(\n",
    "                    xtickstep=10,\n",
    "                    ytickstep=2,\n",
    "                    title='Precedence, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                    ylabel='layer * time',\n",
    "                    xlabel='mem_slot'\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if args.memory_type == 'sdnc':\n",
    "            viz.heatmap(\n",
    "                v['read_positions'],\n",
    "                opts=dict(\n",
    "                    xtickstep=10,\n",
    "                    ytickstep=2,\n",
    "                    title='Read Positions, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                    ylabel='layer * time',\n",
    "                    xlabel='mem_slot'\n",
    "                )\n",
    "            )\n",
    "\n",
    "            viz.heatmap(\n",
    "              v['read_weights'],\n",
    "              opts=dict(\n",
    "                  xtickstep=10,\n",
    "                  ytickstep=2,\n",
    "                  title='Read Weights, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                  ylabel='layer * time',\n",
    "                  xlabel='nr_read_heads * mem_slot'\n",
    "              )\n",
    "          )\n",
    "\n",
    "            viz.heatmap(\n",
    "              v['write_weights'],\n",
    "              opts=dict(\n",
    "                  xtickstep=10,\n",
    "                  ytickstep=2,\n",
    "                  title='Write Weights, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                  ylabel='layer * time',\n",
    "                  xlabel='mem_slot'\n",
    "              )\n",
    "          )\n",
    "\n",
    "            viz.heatmap(\n",
    "              v['usage_vector'] if args.memory_type == 'dnc' else v['usage'],\n",
    "              opts=dict(\n",
    "                  xtickstep=10,\n",
    "                  ytickstep=2,\n",
    "                  title='Usage Vector, t: ' + str(epoch) + ', loss: ' + str(loss),\n",
    "                  ylabel='layer * time',\n",
    "                  xlabel='mem_slot'\n",
    "              )\n",
    "          )\n",
    "\n",
    "    if increment_curriculum:\n",
    "        sequence_max_length = sequence_max_length + args.curriculum_increment\n",
    "        print(\"Increasing max length to \" + str(sequence_max_length))\n",
    "\n",
    "    if take_checkpoint:\n",
    "        llprint(\"\\nSaving Checkpoint ... \"),\n",
    "        check_ptr = os.path.join(ckpts_dir, 'step_{}.pth'.format(epoch))\n",
    "        cur_weights = rnn.state_dict()\n",
    "        T.save(cur_weights, check_ptr)\n",
    "        llprint(\"Done!\\n\")\n",
    "\n",
    "for i in range(int((iterations + 1) / 10)):\n",
    "    llprint(\"\\nIteration %d/%d\" % (i, iterations))\n",
    "    # We test now the learned generalization using sequence_max_length examples\n",
    "    random_length = np.random.randint(2, sequence_max_length * 10 + 1)\n",
    "    input_data, target_output, loss_weights = generate_data(random_length, input_size)\n",
    "\n",
    "    if rnn.debug:\n",
    "        output, (chx, mhx, rv), v = rnn(input_data, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "    else:\n",
    "        output, (chx, mhx, rv) = rnn(input_data, (None, mhx, None), reset_experience=True, pass_through_memory=True)\n",
    "\n",
    "    output = output[:, -1, :].sum().data.cpu().numpy()[0]\n",
    "    target_output = target_output.sum().data.cpu().numpy()\n",
    "\n",
    "try:\n",
    "    print(\"\\nReal value: \", ' = ' + str(int(target_output[0])))\n",
    "    print(\"Predicted:  \", ' = ' + str(int(output // 1)) + \" [\" + str(output) + \"]\")\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
